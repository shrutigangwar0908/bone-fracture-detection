{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBwKtje_f3qu",
        "outputId": "4cc6562e-1b82-47b9-9cba-df4f8177adfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using: cpu\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "mount failed",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4054716415.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# ============================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# ðŸ”¹ Assume data folder is in Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 1. Setup\n",
        "# ============================================================\n",
        "!pip install -q timm grad-cam==1.4.5 opencv-python matplotlib tqdm\n",
        "\n",
        "import torch, timm\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using:\", DEVICE)\n",
        "\n",
        "# ============================================================\n",
        "# 2. Dataset (upload or mount Google Drive)\n",
        "# ============================================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ðŸ”¹ Assume data folder is in Google Drive\n",
        "DATA_DIR = \"/content/drive/MyDrive/fracture_data\"\n",
        "\n",
        "# Structure:\n",
        "# fracture_data/\n",
        "#   train/fracture/*.jpg\n",
        "#   train/normal/*.jpg\n",
        "#   val/fracture/*.jpg\n",
        "#   val/normal/*.jpg\n",
        "\n",
        "IMG_SIZE = 384\n",
        "BATCH = 16\n",
        "\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomRotation(8),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "val_tf = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "train_ds = datasets.ImageFolder(os.path.join(DATA_DIR,\"train\"), transform=train_tf)\n",
        "val_ds   = datasets.ImageFolder(os.path.join(DATA_DIR,\"val\"), transform=val_tf)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True, num_workers=2)\n",
        "val_loader   = DataLoader(val_ds, batch_size=BATCH, shuffle=False, num_workers=2)\n",
        "\n",
        "print(\"Classes:\", train_ds.classes)\n",
        "\n",
        "# ============================================================\n",
        "# 3. Model\n",
        "# ============================================================\n",
        "model = timm.create_model('efficientnet_b0', pretrained=True, num_classes=len(train_ds.classes))\n",
        "model.to(DEVICE)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2)\n",
        "\n",
        "# ============================================================\n",
        "# 4. Training\n",
        "# ============================================================\n",
        "EPOCHS = 5\n",
        "best_acc = 0.0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for imgs, labels in tqdm(train_loader, desc=f\"Train {epoch+1}/{EPOCHS}\"):\n",
        "        imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * imgs.size(0)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in val_loader:\n",
        "            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = model(imgs)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds==labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    acc = correct/total\n",
        "    print(f\"Epoch {epoch+1}, Val Acc: {acc:.4f}\")\n",
        "\n",
        "    scheduler.step(acc)\n",
        "    if acc > best_acc:\n",
        "        best_acc = acc\n",
        "        torch.save({'model_state': model.state_dict(), 'classes': train_ds.classes}, \"best_model.pth\")\n",
        "        print(\"âœ… Saved best model\")\n",
        "\n",
        "print(\"Training done. Best val acc:\", best_acc)\n",
        "\n",
        "# ============================================================\n",
        "# 5. Grad-CAM Visualization\n",
        "# ============================================================\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "from PIL import Image\n",
        "\n",
        "# Reload best model\n",
        "ckpt = torch.load(\"best_model.pth\", map_location=DEVICE)\n",
        "classes = ckpt['classes']\n",
        "model.load_state_dict(ckpt['model_state'])\n",
        "model.eval()\n",
        "\n",
        "# Target layer for CAM\n",
        "target_layer = None\n",
        "for m in reversed(list(model.modules())):\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        target_layer = m\n",
        "        break\n",
        "\n",
        "cam = GradCAM(model=model, target_layers=[target_layer], use_cuda=(DEVICE==\"cuda\"))\n",
        "\n",
        "# Pick one image from val set\n",
        "img_path, _ = val_ds.samples[0]\n",
        "img = Image.open(img_path).convert(\"RGB\")\n",
        "img_resized = img.resize((IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "tf = val_tf\n",
        "input_tensor = tf(img).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "# Prediction\n",
        "out = model(input_tensor)\n",
        "pred_idx = out.argmax(1).item()\n",
        "print(\"Prediction:\", classes[pred_idx])\n",
        "\n",
        "# CAM\n",
        "rgb_np = np.array(img_resized).astype(np.float32)/255.0\n",
        "grayscale_cam = cam(input_tensor=input_tensor)[0]\n",
        "cam_image = show_cam_on_image(rgb_np, grayscale_cam, use_rgb=True)\n",
        "\n",
        "plt.imshow(cam_image)\n",
        "plt.title(f\"Predicted: {classes[pred_idx]}\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8bdfvtjrfe8Y"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 6. Gradio Web App for Demo\n",
        "# ============================================================\n",
        "!pip install -q gradio\n",
        "\n",
        "import gradio as gr\n",
        "import cv2\n",
        "\n",
        "# Reload trained model\n",
        "ckpt = torch.load(\"best_model.pth\", map_location=DEVICE)\n",
        "classes = ckpt['classes']\n",
        "model.load_state_dict(ckpt['model_state'])\n",
        "model.eval()\n",
        "\n",
        "# Grad-CAM setup\n",
        "target_layer = None\n",
        "for m in reversed(list(model.modules())):\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        target_layer = m\n",
        "        break\n",
        "\n",
        "cam = GradCAM(model=model, target_layers=[target_layer], use_cuda=(DEVICE==\"cuda\"))\n",
        "\n",
        "tf = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "def predict_and_cam(image):\n",
        "    # Convert PIL â†’ model input\n",
        "    img_resized = image.resize((IMG_SIZE, IMG_SIZE))\n",
        "    input_tensor = tf(image).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    # Model prediction\n",
        "    with torch.no_grad():\n",
        "        out = model(input_tensor)\n",
        "        probs = torch.softmax(out, dim=1).cpu().numpy()[0]\n",
        "        pred_idx = int(out.argmax(1).item())\n",
        "        pred_label = classes[pred_idx]\n",
        "        confidence = float(probs[pred_idx])\n",
        "\n",
        "    # Grad-CAM\n",
        "    rgb_np = np.array(img_resized).astype(np.float32)/255.0\n",
        "    grayscale_cam = cam(input_tensor=input_tensor)[0]\n",
        "    cam_image = show_cam_on_image(rgb_np, grayscale_cam, use_rgb=True)\n",
        "\n",
        "    # Convert back to PIL for Gradio\n",
        "    cam_pil = Image.fromarray(cam_image)\n",
        "\n",
        "    return {pred_label: confidence}, cam_pil\n",
        "\n",
        "# Gradio UI\n",
        "demo = gr.Interface(\n",
        "    fn=predict_and_cam,\n",
        "    inputs=gr.Image(type=\"pil\", label=\"Upload X-ray\"),\n",
        "    outputs=[gr.Label(num_top_classes=2, label=\"Prediction\"),\n",
        "             gr.Image(type=\"pil\", label=\"Grad-CAM Overlay\")],\n",
        "    title=\"ðŸ©» Bone Fracture Detection\",\n",
        "    description=\"Upload an X-ray to classify fracture vs normal and see Grad-CAM explanation.\"\n",
        ")\n",
        "\n",
        "demo.launch(share=True)  # share=True gives you public link\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4il6V_zMl8LJ"
      },
      "outputs": [],
      "source": [
        "  {\n",
        "   \"cell_type\": \"markdown\",\n",
        "   \"metadata\": {},\n",
        "   \"source\": [\n",
        "    \"## ðŸ§  Classification + Grad-CAM (Explainability)\\n\",\n",
        "    \"This section trains a CNN classifier to detect fracture vs normal, and uses Grad-CAM for explainability.\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": null,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"# Install required libraries\\n\",\n",
        "    \"!pip install -q torch torchvision timm grad-cam\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": null,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"import torch, torchvision\\n\",\n",
        "    \"import torch.nn as nn\\n\",\n",
        "    \"import torch.optim as optim\\n\",\n",
        "    \"from torchvision import transforms, datasets, models\\n\",\n",
        "    \"from torch.utils.data import DataLoader\\n\",\n",
        "    \"from pytorch_grad_cam import GradCAM\\n\",\n",
        "    \"from pytorch_grad_cam.utils.image import show_cam_on_image\\n\",\n",
        "    \"import numpy as np\\n\",\n",
        "    \"from PIL import Image\\n\",\n",
        "    \"\\n\",\n",
        "    \"DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\\n\",\n",
        "    \"IMG_SIZE = 224\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": null,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"# Data transforms\\n\",\n",
        "    \"train_tf = transforms.Compose([\\n\",\n",
        "    \"    transforms.Resize((IMG_SIZE, IMG_SIZE)),\\n\",\n",
        "    \"    transforms.ToTensor(),\\n\",\n",
        "    \"    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\\n\",\n",
        "    \"])\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Example: reuse fracatlas/train/images but need subfolders fracture/normal\\n\",\n",
        "    \"# For demo purpose, assume fracture/normal subfolders already exist\\n\",\n",
        "    \"train_dataset = datasets.ImageFolder(root=\\\"/content/fracatlas/train_class/\\\", transform=train_tf)\\n\",\n",
        "    \"val_dataset   = datasets.ImageFolder(root=\\\"/content/fracatlas/val_class/\\\", transform=train_tf)\\n\",\n",
        "    \"\\n\",\n",
        "    \"train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\\n\",\n",
        "    \"val_loader   = DataLoader(val_dataset, batch_size=16, shuffle=False)\\n\",\n",
        "    \"\\n\",\n",
        "    \"class_names = train_dataset.classes\\n\",\n",
        "    \"print(\\\"Classes:\\\", class_names)\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": null,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"# Model (ResNet18 fine-tune)\\n\",\n",
        "    \"model_cls = models.resnet18(pretrained=True)\\n\",\n",
        "    \"num_ftrs = model_cls.fc.in_features\\n\",\n",
        "    \"model_cls.fc = nn.Linear(num_ftrs, len(class_names))\\n\",\n",
        "    \"model_cls = model_cls.to(DEVICE)\\n\",\n",
        "    \"\\n\",\n",
        "    \"criterion = nn.CrossEntropyLoss()\\n\",\n",
        "    \"optimizer = optim.Adam(model_cls.parameters(), lr=1e-4)\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": null,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"# Train for few epochs\\n\",\n",
        "    \"for epoch in range(2):  # keep small for demo\\n\",\n",
        "    \"    model_cls.train()\\n\",\n",
        "    \"    total_loss = 0\\n\",\n",
        "    \"    for inputs, labels in train_loader:\\n\",\n",
        "    \"        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\\n\",\n",
        "    \"        optimizer.zero_grad()\\n\",\n",
        "    \"        outputs = model_cls(inputs)\\n\",\n",
        "    \"        loss = criterion(outputs, labels)\\n\",\n",
        "    \"        loss.backward()\\n\",\n",
        "    \"        optimizer.step()\\n\",\n",
        "    \"        total_loss += loss.item()\\n\",\n",
        "    \"    print(f\\\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\\\")\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": null,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"# Grad-CAM demo\\n\",\n",
        "    \"target_layer = model_cls.layer4[-1]\\n\",\n",
        "    \"cam = GradCAM(model=model_cls, target_layers=[target_layer], use_cuda=(DEVICE=='cuda'))\\n\",\n",
        "    \"\\n\",\n",
        "    \"def predict_and_cam(image):\\n\",\n",
        "    \"    img_resized = image.resize((IMG_SIZE, IMG_SIZE))\\n\",\n",
        "    \"    input_tensor = train_tf(img_resized).unsqueeze(0).to(DEVICE)\\n\",\n",
        "    \"    with torch.no_grad():\\n\",\n",
        "    \"        out = model_cls(input_tensor)\\n\",\n",
        "    \"        probs = torch.softmax(out, dim=1).cpu().numpy()[0]\\n\",\n",
        "    \"        pred_idx = int(out.argmax(1).item())\\n\",\n",
        "    \"        pred_label = class_names[pred_idx]\\n\",\n",
        "    \"        conf = float(probs[pred_idx])\\n\",\n",
        "    \"\\n\",\n",
        "    \"    rgb_np = np.array(img_resized).astype(np.float32)/255.0\\n\",\n",
        "    \"    grayscale_cam = cam(input_tensor=input_tensor)[0]\\n\",\n",
        "    \"    cam_image = show_cam_on_image(rgb_np, grayscale_cam, use_rgb=True)\\n\",\n",
        "    \"    return {pred_label: conf}, Image.fromarray(cam_image)\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": null,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"# Gradio App for Classification\\n\",\n",
        "    \"demo_cls = gr.Interface(\\n\",\n",
        "    \"    fn=predict_and_cam,\\n\",\n",
        "    \"    inputs=gr.Image(type=\\\"pil\\\", label=\\\"Upload X-ray\\\"),\\n\",\n",
        "    \"    outputs=[gr.Label(num_top_classes=2, label=\\\"Prediction\\\"), gr.Image(type=\\\"pil\\\", label=\\\"Grad-CAM\\\")],\\n\",\n",
        "    \"    title=\\\"ðŸ§  Fracture Classification + Grad-CAM\\\",\\n\",\n",
        "    \"    description=\\\"Upload X-ray â†’ model predicts fracture vs normal with explainability.\\\"\\n\",\n",
        "    \")\\n\",\n",
        "    \"demo_cls.launch(share=True)\"\n",
        "   ]\n",
        "  },\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2dyvlvGQg-l1"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 7. YOLOv8 Bone Fracture Detection\n",
        "# ============================================================\n",
        "!pip install -q ultralytics\n",
        "\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# âœ… Step 1: Load pretrained YOLOv8 model\n",
        "# You can also train on your custom dataset if you have fracture annotations\n",
        "model_yolo = YOLO(\"yolov8n.pt\")  # using small YOLOv8n for demo\n",
        "\n",
        "# âœ… Step 2: Inference on sample image\n",
        "# Replace \"xray.jpg\" with your own fracture X-ray image\n",
        "results = model_yolo.predict(\"xray.jpg\", imgsz=640, conf=0.25)\n",
        "\n",
        "# âœ… Step 3: Show detection results\n",
        "results[0].show()  # display in notebook\n",
        "results[0].save(save_dir=\"yolo_results\")  # save annotated image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xDqC8nCghf7e"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 8. Gradio Web App for YOLOv8\n",
        "# ============================================================\n",
        "\n",
        "import gradio as gr\n",
        "from PIL import Image\n",
        "\n",
        "def yolo_detect(image):\n",
        "    # Save uploaded image temporarily\n",
        "    image.save(\"temp_xray.jpg\")\n",
        "\n",
        "    # Run YOLOv8 inference\n",
        "    results = model_yolo.predict(\"temp_xray.jpg\", imgsz=640, conf=0.25)\n",
        "    annotated = results[0].plot()  # numpy array with boxes\n",
        "\n",
        "    return Image.fromarray(annotated)\n",
        "\n",
        "demo_yolo = gr.Interface(\n",
        "    fn=yolo_detect,\n",
        "    inputs=gr.Image(type=\"pil\", label=\"Upload X-ray\"),\n",
        "    outputs=gr.Image(type=\"pil\", label=\"Fracture Detection\"),\n",
        "    title=\"ðŸ¦´ YOLOv8 Bone Fracture Detection\",\n",
        "    description=\"YOLOv8 object detection model to localize fractures with bounding boxes.\"\n",
        ")\n",
        "\n",
        "demo_yolo.launch(share=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Reset working directory\n",
        "%cd /content\n",
        "\n",
        "# 2. Purana repo folder delete karo\n",
        "!rm -rf /content/bone-fracture-detection\n",
        "\n",
        "# 3. Repo clone karo\n",
        "!git clone https://github.com/shrutigangwar0908/bone-fracture-detection.git\n",
        "%cd bone-fracture-detection\n",
        "\n",
        "# 4. Apna notebook copy karo\n",
        "!cp /content/Untitled1.ipynb /content/bone-fracture-detection/\n",
        "\n",
        "# 5. Git setup + commit + push\n",
        "!git config --global user.email \"YOUR_EMAIL@example.com\"\n",
        "!git config --global user.name \"shrutigangwar0908\"\n",
        "\n",
        "!git checkout -b main\n",
        "!git add .\n",
        "!git commit -m \"Final upload - Bone Fracture Detection notebook\"\n",
        "!git push origin main\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFD6_kcGcewm",
        "outputId": "f0f4585d-1669-4d4e-9b2c-dc1e4dc6f348"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'bone-fracture-detection'...\n",
            "warning: You appear to have cloned an empty repository.\n",
            "/content/bone-fracture-detection\n",
            "cp: cannot stat '/content/Untitled1.ipynb': No such file or directory\n",
            "Switched to a new branch 'main'\n",
            "On branch main\n",
            "\n",
            "Initial commit\n",
            "\n",
            "nothing to commit (create/copy files and use \"git add\" to track)\n",
            "error: src refspec main does not match any\n",
            "\u001b[31merror: failed to push some refs to 'https://github.com/shrutigangwar0908/bone-fracture-detection.git'\n",
            "\u001b[m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvitREP2eH7t",
        "outputId": "9437119a-f977-4214-ff85-7446dbe9ab39"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bone-fracture-detection  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "FqRakaDyeyi7"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}